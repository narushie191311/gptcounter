#!/usr/bin/env python3
"""
Colabç©¶æ¥µæ€§èƒ½ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æœ€é«˜å“è³ªãƒ»æœ€é€Ÿå‡¦ç† + ETAè‡ªå‹•èª¿æ•´æ©Ÿèƒ½
"""

import os
import sys
import subprocess
import time
import multiprocessing as mp
from pathlib import Path
import argparse
import json
import psutil
import GPUtil
import threading
import queue

def get_gpu_info():
    """GPUæƒ…å ±ã‚’å–å¾—"""
    try:
        gpus = GPUtil.getGPUs()
        if gpus:
            print(f"åˆ©ç”¨å¯èƒ½GPUæ•°: {len(gpus)}")
            for i, gpu in enumerate(gpus):
                print(f"GPU {i}: {gpu.name}")
                print(f"  ãƒ¡ãƒ¢ãƒª: {gpu.memoryTotal}MB")
                print(f"  ä½¿ç”¨ç‡: {gpu.load*100:.1f}%")
                print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {gpu.memoryUsed}/{gpu.memoryTotal}MB")
        return len(gpus)
    except:
        print("GPUæƒ…å ±ã®å–å¾—ã«å¤±æ•—")
        return 0

def install_ultimate_dependencies():
    """ç©¶æ¥µæ€§èƒ½ç”¨ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("=== ç©¶æ¥µæ€§èƒ½ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ===")
    
    # é‡ã„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚OK
    packages = [
        # åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
        "torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118",
        "ultralytics",
        "opencv-python",
        "numpy",
        "pandas",
        "av>=10.0.0",
        "insightface",
        "onnxruntime-gpu",
        "supervision",
        "annoy",
        "gdown",
        
        # ç©¶æ¥µæ€§èƒ½ç”¨
        "tensorrt",  # TensorRT
        "nvidia-ml-py3",  # GPUç›£è¦–
        "psutil",  # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
        "GPUtil",  # GPUç›£è¦–
        "joblib",  # ä¸¦åˆ—å‡¦ç†
        "tqdm",  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        "scikit-learn",  # æ©Ÿæ¢°å­¦ç¿’
        "scipy",  # ç§‘å­¦è¨ˆç®—
        "matplotlib",  # å¯è¦–åŒ–
        "seaborn",  # çµ±è¨ˆå¯è¦–åŒ–
    ]
    
    for pkg in packages:
        try:
            print(f"ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­: {pkg}")
            subprocess.run(f"pip install -q {pkg}", shell=True, check=False)
            print(f"âœ“ {pkg}")
        except:
            print(f"âœ— {pkg} ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—")

def create_ultimate_config():
    """ç©¶æ¥µæ€§èƒ½è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    config = {
        "ultimate": {
            "det_size": "2048x2048",  # æœ€é«˜è§£åƒåº¦
            "yolo_weights": "yolov8x.pt",  # æœ€å¤§ãƒ¢ãƒ‡ãƒ«
            "reid_backend": "ensemble",
            "face_model": "buffalo_l",
            "gait_features": True,
            "detect_every_n": 1,
            "log_every_sec": 2,
            "checkpoint_every_sec": 10,
            "merge_every_sec": 30,
            "flush_every_n": 5,
            "use_tensorrt": True,
            "fp16_inference": True,
            "batch_size": 8,
            "num_workers": 8,
            "mixed_precision": True,
            "gradient_checkpointing": True
        },
        "high_performance": {
            "det_size": "1536x1536",
            "yolo_weights": "yolov8x.pt",
            "reid_backend": "ensemble",
            "face_model": "buffalo_l",
            "gait_features": True,
            "detect_every_n": 1,
            "log_every_sec": 5,
            "checkpoint_every_sec": 15,
            "merge_every_sec": 60,
            "flush_every_n": 10,
            "use_tensorrt": True,
            "fp16_inference": True,
            "batch_size": 4,
            "num_workers": 4
        },
        "balanced": {
            "det_size": "1280x1280",
            "yolo_weights": "yolov8m.pt",
            "reid_backend": "ensemble",
            "face_model": "buffalo_l",
            "gait_features": True,
            "detect_every_n": 1,
            "log_every_sec": 10,
            "checkpoint_every_sec": 30,
            "merge_every_sec": 120,
            "flush_every_n": 20,
            "use_tensorrt": False,
            "fp16_inference": True,
            "batch_size": 2,
            "num_workers": 2
        },
        "fast": {
            "det_size": "960x960",
            "yolo_weights": "yolov8n.pt",
            "reid_backend": "hist",
            "face_model": "buffalo_l",
            "gait_features": False,
            "detect_every_n": 2,
            "log_every_sec": 15,
            "checkpoint_every_sec": 60,
            "merge_every_sec": 300,
            "flush_every_n": 50,
            "use_tensorrt": False,
            "fp16_inference": False,
            "batch_size": 1,
            "num_workers": 1
        }
    }
    
    with open("colab_ultimate_config.json", "w") as f:
        json.dump(config, f, indent=2)
    
    print("âœ“ ç©¶æ¥µæ€§èƒ½è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: colab_ultimate_config.json ã‚’ä½œæˆ")

def create_eta_auto_adjuster():
    """ETAè‡ªå‹•èª¿æ•´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    script_content = '''#!/usr/bin/env python3
"""
ETAè‡ªå‹•èª¿æ•´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ™‚é–“åˆ¶ç´„å†…ã§æœ€é«˜å“è³ªã‚’ç¶­æŒ
"""

import os
import sys
import subprocess
import time
import json
import cv2
from pathlib import Path
import argparse
import threading
import queue

class ETAAdjuster:
    def __init__(self, video_path, target_time, quality_priority=0.8):
        self.video_path = video_path
        self.target_time = target_time  # ç›®æ¨™æ™‚é–“ï¼ˆç§’ï¼‰
        self.quality_priority = quality_priority  # å“è³ªå„ªå…ˆåº¦ (0.0-1.0)
        self.video_info = self._get_video_info()
        
    def _get_video_info(self):
        """å‹•ç”»æƒ…å ±ã‚’å–å¾—"""
        cap = cv2.VideoCapture(self.video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_duration = total_frames / fps
        cap.release()
        
        return {
            "total_frames": total_frames,
            "fps": fps,
            "total_duration": total_duration
        }
    
    def estimate_processing_time(self, config):
        """å‡¦ç†æ™‚é–“ã‚’æ¨å®š"""
        # è§£åƒåº¦ã«ã‚ˆã‚‹å‡¦ç†æ™‚é–“ã®é‡ã¿
        size_weights = {
            "640x640": 1.0,
            "960x960": 1.5,
            "1280x1280": 2.5,
            "1536x1536": 4.0,
            "2048x2048": 6.0
        }
        
        # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å‡¦ç†æ™‚é–“ã®é‡ã¿
        model_weights = {
            "yolov8n.pt": 1.0,
            "yolov8m.pt": 1.8,
            "yolov8x.pt": 3.0
        }
        
        # åŸºæœ¬å‡¦ç†æ™‚é–“ï¼ˆ1åˆ†é–“ã®å‹•ç”»ã‚’640x640ã§å‡¦ç†ã™ã‚‹å ´åˆï¼‰
        base_time = 60  # ç§’
        
        # é‡ã¿ä»˜ã‘è¨ˆç®—
        size_weight = size_weights.get(config["det_size"], 2.0)
        model_weight = model_weights.get(config["yolo_weights"], 1.5)
        
        # æ¨å®šå‡¦ç†æ™‚é–“
        estimated_time = (self.video_info["total_duration"] / 60) * base_time * size_weight * model_weight
        
        return estimated_time
    
    def find_optimal_config(self):
        """æœ€é©ãªè¨­å®šã‚’è¦‹ã¤ã‘ã‚‹"""
        with open("colab_ultimate_config.json", "r") as f:
            configs = json.load(f)
        
        best_config = None
        best_score = -1
        
        for config_name, config in configs.items():
            estimated_time = self.estimate_processing_time(config)
            
            # æ™‚é–“åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
            if estimated_time > self.target_time:
                continue
            
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_score = self._calculate_quality_score(config)
            
            # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆå“è³ªå„ªå…ˆåº¦ã‚’è€ƒæ…®ï¼‰
            total_score = quality_score * self.quality_priority + (1 - estimated_time/self.target_time) * (1 - self.quality_priority)
            
            if total_score > best_score:
                best_score = total_score
                best_config = config_name
        
        return best_config, configs.get(best_config) if best_config else None
    
    def _calculate_quality_score(self, config):
        """å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0.0
        
        # è§£åƒåº¦ã‚¹ã‚³ã‚¢
        size_scores = {
            "640x640": 0.3,
            "960x960": 0.5,
            "1280x1280": 0.7,
            "1536x1536": 0.9,
            "2048x2048": 1.0
        }
        score += size_scores.get(config["det_size"], 0.5)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢
        model_scores = {
            "yolov8n.pt": 0.6,
            "yolov8m.pt": 0.8,
            "yolov8x.pt": 1.0
        }
        score += model_scores.get(config["yolo_weights"], 0.7)
        
        # æ©Ÿèƒ½ã‚¹ã‚³ã‚¢
        if config.get("gait_features"):
            score += 0.1
        if config.get("use_tensorrt"):
            score += 0.1
        if config.get("fp16_inference"):
            score += 0.05
        
        return min(score / 2.25, 1.0)  # æ­£è¦åŒ–
    
    def auto_adjust_and_execute(self):
        """è‡ªå‹•èª¿æ•´ã—ã¦å®Ÿè¡Œ"""
        print(f"=== ETAè‡ªå‹•èª¿æ•´é–‹å§‹ ===")
        print(f"å‹•ç”»é•·: {self.video_info['total_duration']:.1f} ç§’")
        print(f"ç›®æ¨™æ™‚é–“: {self.target_time:.1f} ç§’")
        print(f"å“è³ªå„ªå…ˆåº¦: {self.quality_priority:.2f}")
        
        # æœ€é©è¨­å®šã‚’æ¤œç´¢
        best_config_name, best_config = self.find_optimal_config()
        
        if not best_config:
            print("âœ— æ™‚é–“åˆ¶ç´„å†…ã§å®Ÿè¡Œå¯èƒ½ãªè¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("å“è³ªã‚’ä¸‹ã’ã‚‹ã‹ã€æ™‚é–“ã‚’å»¶é•·ã—ã¦ãã ã•ã„")
            return False
        
        estimated_time = self.estimate_processing_time(best_config)
        quality_score = self._calculate_quality_score(best_config)
        
        print(f"\\nğŸ¯ æœ€é©è¨­å®š: {best_config_name}")
        print(f"æ¨å®šå‡¦ç†æ™‚é–“: {estimated_time:.1f} ç§’")
        print(f"å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.2f}")
        print(f"æ™‚é–“ä½™è£•: {self.target_time - estimated_time:.1f} ç§’")
        
        # å®Ÿè¡Œç¢ºèª
        response = input("\\nã“ã®è¨­å®šã§å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
        if response.lower() != 'y':
            print("å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            return False
        
        # å®Ÿè¡Œ
        return self._execute_with_config(best_config)
    
    def _execute_with_config(self, config):
        """è¨­å®šã§å®Ÿè¡Œ"""
        print(f"\\nğŸš€ å®Ÿè¡Œé–‹å§‹: {config['det_size']}, {config['yolo_weights']}")
        
        # ä¸¦åˆ—å‡¦ç†ã§å®Ÿè¡Œ
        cmd = f'''python scripts/colab_parallel.py \\
          --video "{self.video_path}" \\
          --chunks 4 \\
          --config {self._get_config_name(config)} \\
          --eta-target {self.target_time}'''
        
        try:
            result = subprocess.run(cmd, shell=True)
            return result.returncode == 0
        except Exception as e:
            print(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _get_config_name(self, config):
        """è¨­å®šåã‚’å–å¾—"""
        # è¨­å®šã‹ã‚‰åå‰ã‚’é€†å¼•ã
        with open("colab_ultimate_config.json", "r") as f:
            configs = json.load(f)
        
        for name, cfg in configs.items():
            if cfg == config:
                return name
        
        return "balanced"

def main():
    parser = argparse.ArgumentParser(description="ETAè‡ªå‹•èª¿æ•´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--video", required=True, help="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--target-time", type=float, required=True, help="ç›®æ¨™å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--quality-priority", type=float, default=0.8, help="å“è³ªå„ªå…ˆåº¦ (0.0-1.0)")
    
    args = parser.parse_args()
    
    adjuster = ETAAdjuster(args.video, args.target_time, args.quality_priority)
    success = adjuster.auto_adjust_and_execute()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
'''
    
    with open("scripts/eta_auto_adjuster.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ“ ETAè‡ªå‹•èª¿æ•´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: scripts/eta_auto_adjuster.py ã‚’ä½œæˆ")

def create_ultimate_parallel_processor():
    """ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    script_content = '''#!/usr/bin/env python3
"""
ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ETAåˆ¶å¾¡ä»˜ãã®æœ€é«˜å“è³ªå‡¦ç†
"""

import os
import sys
import subprocess
import multiprocessing as mp
import json
import time
import threading
from pathlib import Path
import argparse
import psutil
import GPUtil

class UltimateParallelProcessor:
    def __init__(self, video_path, num_chunks, config_name, eta_target=None):
        self.video_path = video_path
        self.num_chunks = num_chunks
        self.config_name = config_name
        self.eta_target = eta_target
        self.start_time = None
        self.chunk_results = []
        self.lock = threading.Lock()
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        with open("colab_ultimate_config.json", "r") as f:
            self.configs = json.load(f)
        
        self.config = self.configs[config_name]
    
    def process_chunk_with_monitoring(self, args):
        """ç›£è¦–ä»˜ããƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        chunk_id, start_sec, duration_sec = args
        
        # GPUä½¿ç”¨ç‡ç›£è¦–
        gpu_monitor = GPUMonitor()
        
        output_csv = f"outputs/chunks/chunk_{chunk_id:03d}.csv"
        output_video = f"outputs/chunks/chunk_{chunk_id:03d}.mp4"
        
        cmd = self._build_chunk_command(chunk_id, start_sec, duration_sec, output_csv, output_video)
        
        print(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: {start_sec}s - {start_sec + duration_sec}s é–‹å§‹")
        
        try:
            # å‡¦ç†é–‹å§‹
            process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
            while process.poll() is None:
                gpu_stats = gpu_monitor.get_stats()
                
                # GPUãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®å¯¾å¿œ
                if gpu_stats["memory_used"] > gpu_stats["memory_total"] * 0.9:
                    print(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: GPUãƒ¡ãƒ¢ãƒªä¸è¶³ - å‡¦ç†ã‚’ä¸€æ™‚åœæ­¢")
                    process.terminate()
                    time.sleep(5)
                    # è»½é‡è¨­å®šã§å†è©¦è¡Œ
                    return self._retry_with_light_config(chunk_id, start_sec, duration_sec)
                
                time.sleep(2)
            
            if process.returncode == 0:
                print(f"âœ“ ãƒãƒ£ãƒ³ã‚¯ {chunk_id} å®Œäº†")
                with self.lock:
                    self.chunk_results.append((True, chunk_id, output_csv))
                return True, chunk_id, output_csv
            else:
                print(f"âœ— ãƒãƒ£ãƒ³ã‚¯ {chunk_id} å¤±æ•—")
                return False, chunk_id, None
                
        except Exception as e:
            print(f"âœ— ãƒãƒ£ãƒ³ã‚¯ {chunk_id} ã‚¨ãƒ©ãƒ¼: {e}")
            return False, chunk_id, None
    
    def _build_chunk_command(self, chunk_id, start_sec, duration_sec, output_csv, output_video):
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰"""
        cmd = f'''python scripts/analyze_video_mac.py \\
          --video "{self.video_path}" \\
          --start-sec {start_sec} \\
          --duration-sec {duration_sec} \\
          --output-csv {output_csv} \\
          --device cuda \\
          --yolo-weights {self.config['yolo_weights']} \\
          --reid-backend {self.config['reid_backend']} \\
          --face-model {self.config['face_model']} \\
          --det-size {self.config['det_size']} \\
          --detect-every-n {self.config['detect_every_n']} \\
          --log-every-sec {self.config['log_every_sec']} \\
          --checkpoint-every-sec {self.config['checkpoint_every_sec']} \\
          --merge-every-sec {self.config['merge_every_sec']} \\
          --flush-every-n {self.config['flush_every_n']} \\
          --save-video --video-out {output_video} \\
          --no-show'''
        
        if self.config.get('gait_features'):
            cmd += " --gait-features"
        
        return cmd
    
    def _retry_with_light_config(self, chunk_id, start_sec, duration_sec):
        """è»½é‡è¨­å®šã§å†è©¦è¡Œ"""
        print(f"ğŸ”„ ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: è»½é‡è¨­å®šã§å†è©¦è¡Œ")
        
        light_config = {
            "yolo_weights": "yolov8n.pt",
            "det_size": "640x640",
            "detect_every_n": 2,
            "gait_features": False
        }
        
        # è»½é‡è¨­å®šã§ã‚³ãƒãƒ³ãƒ‰å†æ§‹ç¯‰
        cmd = f'''python scripts/analyze_video_mac.py \\
          --video "{self.video_path}" \\
          --start-sec {start_sec} \\
          --duration-sec {duration_sec} \\
          --output-csv outputs/chunks/chunk_{chunk_id:03d}_light.csv \\
          --device cuda \\
          --yolo-weights {light_config['yolo_weights']} \\
          --det-size {light_config['det_size']} \\
          --detect-every-n {light_config['detect_every_n']} \\
          --no-show'''
        
        try:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            if result.returncode == 0:
                print(f"âœ“ ãƒãƒ£ãƒ³ã‚¯ {chunk_id} è»½é‡è¨­å®šã§å®Œäº†")
                return True, chunk_id, f"outputs/chunks/chunk_{chunk_id:03d}_light.csv"
            else:
                return False, chunk_id, None
        except:
            return False, chunk_id, None
    
    def execute_with_eta_control(self):
        """ETAåˆ¶å¾¡ä»˜ãã§å®Ÿè¡Œ"""
        self.start_time = time.time()
        
        # å‹•ç”»æƒ…å ±å–å¾—
        video_info = self._get_video_info()
        if not video_info:
            return False
        
        print(f"å‹•ç”»æƒ…å ±: {video_info['total_frames']} ãƒ•ãƒ¬ãƒ¼ãƒ , {video_info['fps']:.2f} FPS, {video_info['total_duration']:.1f} ç§’")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = self._create_chunks(video_info['total_duration'])
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        Path("outputs/chunks").mkdir(parents=True, exist_ok=True)
        
        # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        print(f"ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†é–‹å§‹: {self.num_chunks} ãƒãƒ£ãƒ³ã‚¯")
        
        with mp.Pool(processes=min(self.num_chunks, mp.cpu_count())) as pool:
            results = pool.map(self.process_chunk_with_monitoring, chunks)
        
        # çµæœé›†è¨ˆ
        successful_chunks = []
        for success, chunk_id, output_csv in results:
            if success:
                successful_chunks.append(output_csv)
        
        end_time = time.time()
        processing_time = end_time - self.start_time
        
        print(f"\\nğŸ‰ ä¸¦åˆ—å‡¦ç†å®Œäº†: {len(successful_chunks)}/{self.num_chunks} ãƒãƒ£ãƒ³ã‚¯æˆåŠŸ")
        print(f"å‡¦ç†æ™‚é–“: {processing_time:.1f} ç§’")
        print(f"é€Ÿåº¦: {video_info['total_duration']/processing_time:.2f}x ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ")
        
        # ETAåˆ¶å¾¡çµæœ
        if self.eta_target:
            if processing_time <= self.eta_target:
                print(f"âœ… ç›®æ¨™æ™‚é–“ {self.eta_target:.1f}ç§’ å†…ã§å®Œäº†ï¼")
            else:
                print(f"âš ï¸ ç›®æ¨™æ™‚é–“ {self.eta_target:.1f}ç§’ ã‚’ {processing_time - self.eta_target:.1f}ç§’ è¶…é")
        
        # ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸
        if successful_chunks:
            self._merge_chunks(successful_chunks, "outputs/ultimate_merged_analysis.csv")
        
        return len(successful_chunks) == self.num_chunks
    
    def _get_video_info(self):
        """å‹•ç”»æƒ…å ±ã‚’å–å¾—"""
        try:
            import cv2
            cap = cv2.VideoCapture(self.video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_duration = total_frames / fps
            cap.release()
            return {"total_frames": total_frames, "fps": fps, "total_duration": total_duration}
        except:
            print("å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—")
            return None
    
    def _create_chunks(self, total_duration):
        """ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        chunk_duration = total_duration / self.num_chunks
        chunks = []
        
        for i in range(self.num_chunks):
            start_sec = i * chunk_duration
            duration_sec = chunk_duration
            if i == self.num_chunks - 1:
                duration_sec = total_duration - start_sec
            chunks.append((i, start_sec, duration_sec))
        
        return chunks
    
    def _merge_chunks(self, chunk_files, output_file):
        """ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸"""
        print("ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸ä¸­...")
        
        import pandas as pd
        
        dfs = []
        for chunk_file in chunk_files:
            if os.path.exists(chunk_file):
                df = pd.read_csv(chunk_file)
                dfs.append(df)
        
        if dfs:
            merged_df = pd.concat(dfs, ignore_index=True)
            merged_df.to_csv(output_file, index=False)
            print(f"âœ“ ãƒãƒ¼ã‚¸å®Œäº†: {output_file}")
            print(f"  ç·æ¤œå‡ºæ•°: {len(merged_df)}")
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯äººç‰©æ•°: {merged_df['person_id'].nunique() if 'person_id' in merged_df.columns else 'N/A'}")
        else:
            print("âœ— ãƒãƒ¼ã‚¸ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

class GPUMonitor:
    """GPUç›£è¦–ã‚¯ãƒ©ã‚¹"""
    def get_stats(self):
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # æœ€åˆã®GPU
                return {
                    "memory_used": gpu.memoryUsed,
                    "memory_total": gpu.memoryTotal,
                    "load": gpu.load,
                    "temperature": gpu.temperature
                }
        except:
            pass
        
        return {"memory_used": 0, "memory_total": 1, "load": 0, "temperature": 0}

def main():
    parser = argparse.ArgumentParser(description="ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--video", required=True, help="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--chunks", type=int, default=4, help="ãƒãƒ£ãƒ³ã‚¯æ•°")
    parser.add_argument("--config", default="ultimate", 
                       choices=["ultimate", "high_performance", "balanced", "fast"],
                       help="è¨­å®šãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--eta-target", type=float, help="ç›®æ¨™å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰")
    
    args = parser.parse_args()
    
    processor = UltimateParallelProcessor(args.video, args.chunks, args.config, args.eta_target)
    success = processor.execute_with_eta_control()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
'''
    
    with open("scripts/ultimate_parallel.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ“ ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: scripts/ultimate_parallel.py ã‚’ä½œæˆ")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("=== Colabç©¶æ¥µæ€§èƒ½ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ ===")
    
    # GPUæƒ…å ±ç¢ºèª
    gpu_count = get_gpu_info()
    
    # ç©¶æ¥µæ€§èƒ½ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    install_ultimate_dependencies()
    
    # ç©¶æ¥µæ€§èƒ½è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    create_ultimate_config()
    
    # ETAè‡ªå‹•èª¿æ•´ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    create_eta_auto_adjuster()
    
    # ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    create_ultimate_parallel_processor()
    
    print("\n=== ç©¶æ¥µæ€§èƒ½ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº† ===")
    print("\nğŸš€ æ¨å¥¨å®Ÿè¡Œæ‰‹é †:")
    print("1. gptcounter_colab_ultimate.ipynb ã‚’Colabã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    print("2. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ â†’ GPU (A100æ¨å¥¨)")
    print("3. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‚’å®Ÿè¡Œ")
    print("4. ETAè‡ªå‹•èª¿æ•´ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    print("\nâš¡ ç©¶æ¥µæ€§èƒ½ã®ãƒã‚¤ãƒ³ãƒˆ:")
    print(f"- ä¸¦åˆ—å‡¦ç†: {gpu_count} GPUç’°å¢ƒã§æœ€å¤§{gpu_count*2}ä¸¦åˆ—")
    print("- æœ€é«˜è§£åƒåº¦: 2048x2048ã§ç©¶æ¥µå“è³ª")
    print("- ETAåˆ¶å¾¡: æ™‚é–“åˆ¶ç´„å†…ã§æœ€é«˜å“è³ªã‚’ç¶­æŒ")
    print("- è‡ªå‹•èª¿æ•´: å“è³ªã‚’è½ã¨ã•ãšæ™‚é–“ã‚’å®ˆã‚‹")

if __name__ == "__main__":
    main()
