#!/usr/bin/env python3
"""
ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ETAåˆ¶å¾¡ä»˜ãã®æœ€é«˜å“è³ªå‡¦ç†
"""

import os
import sys
import subprocess
import multiprocessing as mp
import json
import time
import threading
from pathlib import Path
import argparse
import psutil
try:
    import GPUtil  # optional
except Exception:  # pragma: no cover
    GPUtil = None
import signal

class UltimateParallelProcessor:
    def __init__(self, video_path, num_chunks, config_name, eta_target=None, output_csv=None, output_dir=None):
        self.video_path = video_path
        self.num_chunks = num_chunks
        self.config_name = config_name
        self.eta_target = eta_target
        self.output_dir = str(output_dir or "outputs")
        self.chunks_dir = str(Path(self.output_dir) / "chunks")
        Path(self.chunks_dir).mkdir(parents=True, exist_ok=True)
        self.output_csv = output_csv or str(Path(self.output_dir) / "ultimate_merged_analysis.csv")
        self.start_time = None
        self.chunk_results = []
        self.resume_file = str(Path(self.output_dir) / "parallel_resume_state.json")
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆï¼ˆ/content/gptcounter ã®ã‚ˆã†ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
        self.project_root = Path(__file__).resolve().parent.parent
        
        # è¨­å®šèª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ãƒ‘ã‚¹ã‹ã‚‰æ¤œç´¢ï¼‰
        self.configs = self._load_config()
        self.config = self.configs[config_name]
        
        # ä¸­æ–­ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°ãƒ‘ã‚¹ã‹ã‚‰æ¤œç´¢ã—ã¦èª­ã¿è¾¼ã¿"""
        config_paths = [
            "colab_ultimate_config.json",  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            "scripts/colab_ultimate_config.json",  # scriptsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            "../colab_ultimate_config.json",  # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            "/content/drive/MyDrive/gptcounter/colab_ultimate_config.json",  # Colabçµ¶å¯¾ãƒ‘ã‚¹
            "/content/gptcounter/colab_ultimate_config.json",  # Colabæ¨™æº–ãƒ‘ã‚¹
        ]
        
        for config_path in config_paths:
            try:
                if os.path.exists(config_path):
                    print(f"âœ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {config_path}")
                    with open(config_path, "r") as f:
                        return json.load(f)
            except Exception as e:
                print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•— {config_path}: {e}")
                continue
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        print("âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return {
            "ultimate": {
                "det_size": "2048x2048",
                "yolo_weights": "yolov8x.pt",
                "reid_backend": "ensemble",
                "face_model": "buffalo_l",
                "gait_features": True,
                "detect_every_n": 1,
                "log_every_sec": 2,
                "checkpoint_every_sec": 10,
                "merge_every_sec": 30,
                "flush_every_n": 5
            },
            "high_performance": {
                "det_size": "1536x1536",
                "yolo_weights": "yolov8x.pt",
                "reid_backend": "ensemble",
                "face_model": "buffalo_l",
                "gait_features": True,
                "detect_every_n": 1,
                "log_every_sec": 5,
                "checkpoint_every_sec": 15,
                "merge_every_sec": 60,
                "flush_every_n": 10
            },
            "balanced": {
                "det_size": "1280x1280",
                "yolo_weights": "yolov8m.pt",
                "reid_backend": "ensemble",
                "face_model": "buffalo_l",
                "gait_features": True,
                "detect_every_n": 1,
                "log_every_sec": 10,
                "checkpoint_every_sec": 30,
                "merge_every_sec": 120,
                "flush_every_n": 20
            },
            "fast": {
                "det_size": "960x960",
                "yolo_weights": "yolov8n.pt",
                "reid_backend": "hist",
                "face_model": "buffalo_l",
                "gait_features": False,
                "detect_every_n": 2,
                "log_every_sec": 15,
                "checkpoint_every_sec": 60,
                "merge_every_sec": 300,
                "flush_every_n": 50
            }
        }
    
    def _signal_handler(self, signum, frame):
        """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: ä¸­æ–­æ™‚ã«ä¸­æ–­çŠ¶æ…‹ã‚’ä¿å­˜"""
        print(f"\nã‚·ã‚°ãƒŠãƒ« {signum} ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚ä¸­æ–­çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã™...")
        self._save_resume_state()
        sys.exit(0)
    
    def _save_resume_state(self):
        """ä¸­æ–­çŠ¶æ…‹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        resume_data = {
            "video_path": self.video_path,
            "num_chunks": self.num_chunks,
            "config_name": self.config_name,
            "eta_target": self.eta_target,
            "start_time": self.start_time,
            "chunk_results": self.chunk_results
        }
        try:
            with open(self.resume_file, "w") as f:
                json.dump(resume_data, f, indent=4)
            print(f"ä¸­æ–­çŠ¶æ…‹ã‚’ {self.resume_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"ä¸­æ–­çŠ¶æ…‹ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    def _load_resume_state(self):
        """ä¸­æ–­çŠ¶æ…‹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.resume_file):
            try:
                with open(self.resume_file, "r") as f:
                    resume_data = json.load(f)
                    self.video_path = resume_data["video_path"]
                    self.num_chunks = resume_data["num_chunks"]
                    self.config_name = resume_data["config_name"]
                    self.eta_target = resume_data["eta_target"]
                    self.start_time = resume_data["start_time"]
                    self.chunk_results = resume_data["chunk_results"]
                    print(f"ä¸­æ–­çŠ¶æ…‹ã‚’ {self.resume_file} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                    return True
            except Exception as e:
                print(f"ä¸­æ–­çŠ¶æ…‹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return False
    
    def process_chunk_with_monitoring(self, args):
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼ˆã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã®stdout/stderrã‚’å–å¾—ã—ã¦å¯è¦–åŒ–ï¼‰"""
        chunk_id, start_sec, duration_sec = args
        
        output_csv = f"{self.chunks_dir}/chunk_{chunk_id:03d}.csv"
        output_video = f"{self.chunks_dir}/chunk_{chunk_id:03d}.mp4"
        
        cmd = self._build_chunk_command(chunk_id, start_sec, duration_sec, output_csv, output_video)
        
        print(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: {start_sec}s - {start_sec + duration_sec}s é–‹å§‹")
        
        try:
            # çµ¶å¯¾ãƒ‘ã‚¹ + å›ºå®šcwdã§å®Ÿè¡Œï¼ˆç›¸å¯¾ãƒ‘ã‚¹å•é¡Œã‚’å›é¿ï¼‰
            # TensorRTåˆæœŸåŒ–ç«¶åˆã®å›é¿ï¼ˆONNXRuntimeã®TRT EPã‚’ç„¡åŠ¹åŒ–ï¼‰
            env = os.environ.copy()
            env.setdefault("ORT_DISABLE_TENSORRT", "1")
            env.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
            env.setdefault("INSIGHTFACE_HOME", str(self.project_root / "models_insightface" / "models"))
            result = subprocess.run(
                cmd, cwd=str(self.project_root), capture_output=True, text=True, env=env
            )
            if result.returncode == 0:
                print(f"âœ“ ãƒãƒ£ãƒ³ã‚¯ {chunk_id} å®Œäº†")
                if result.stdout:
                    print(result.stdout[:500])
                return True, chunk_id, output_csv
            else:
                print(f"âœ— ãƒãƒ£ãƒ³ã‚¯ {chunk_id} å¤±æ•— (code={result.returncode})")
                if result.stdout:
                    print(f"[stdout]\n{result.stdout[:1000]}")
                if result.stderr:
                    print(f"[stderr]\n{result.stderr[:2000]}")
                return False, chunk_id, None
        except Exception as e:
            print(f"âœ— ãƒãƒ£ãƒ³ã‚¯ {chunk_id} ã‚¨ãƒ©ãƒ¼: {e}")
            return False, chunk_id, None
    
    def _build_chunk_command(self, chunk_id, start_sec, duration_sec, output_csv, output_video):
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚³ãƒãƒ³ãƒ‰ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼‰ã‚’æ§‹ç¯‰ï¼ˆå®‰å…¨ãªå®Ÿè¡Œï¼‰"""
        script_path = str(self.project_root / "scripts" / "analyze_video_mac.py")
        cmd = [
            sys.executable, script_path,
            "--video", str(self.video_path),
            "--start-sec", str(start_sec),
            "--duration-sec", str(duration_sec),
            "--output-csv", str(output_csv),
            "--device", "cuda",
            "--yolo-weights", str(self.config['yolo_weights']),
            "--reid-backend", str(self.config['reid_backend']),
            "--face-model", str(self.config['face_model']),
            "--det-size", str(self.config['det_size']),
            "--detect-every-n", str(self.config['detect_every_n']),
            "--log-every-sec", str(self.config['log_every_sec']),
            "--checkpoint-every-sec", str(self.config['checkpoint_every_sec']),
            "--merge-every-sec", str(self.config['merge_every_sec']),
            "--flush-every-n", str(self.config['flush_every_n']),
            "--save-video", "--video-out", str(output_video),
            "--no-show",
        ]
        if self.config.get('gait_features'):
            cmd.append("--gait-features")
        return cmd
    
    def _retry_with_light_config(self, chunk_id, start_sec, duration_sec):
        """è»½é‡è¨­å®šã§å†è©¦è¡Œ"""
        print(f"ğŸ”„ ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: è»½é‡è¨­å®šã§å†è©¦è¡Œ")
        
        light_config = {
            "yolo_weights": "yolov8n.pt",
            "det_size": "640x640",
            "detect_every_n": 2,
            "gait_features": False
        }
        
        # è»½é‡è¨­å®šã§ã‚³ãƒãƒ³ãƒ‰å†æ§‹ç¯‰
        light_script = str(self.project_root / "scripts" / "analyze_video_mac.py")
        cmd = [
            sys.executable, light_script,
            "--video", str(self.video_path),
            "--start-sec", str(start_sec),
            "--duration-sec", str(duration_sec),
            "--output-csv", f"{self.chunks_dir}/chunk_{chunk_id:03d}_light.csv",
            "--device", "cuda",
            "--yolo-weights", str(light_config['yolo_weights']),
            "--det-size", str(light_config['det_size']),
            "--detect-every-n", str(light_config['detect_every_n']),
            "--no-show",
        ]
        
        try:
            result = subprocess.run(cmd, cwd=str(self.project_root), capture_output=True, text=True)
            if result.returncode == 0:
                print(f"âœ“ ãƒãƒ£ãƒ³ã‚¯ {chunk_id} è»½é‡è¨­å®šã§å®Œäº†")
                return True, chunk_id, f"{self.chunks_dir}/chunk_{chunk_id:03d}_light.csv"
            else:
                if result.stdout:
                    print(f"[stdout]\n{result.stdout[:1000]}")
                if result.stderr:
                    print(f"[stderr]\n{result.stderr[:2000]}")
                return False, chunk_id, None
        except:
            return False, chunk_id, None

    def _ensure_models_downloaded(self):
        """å­ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•å‰ã«å¿…è¦ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€åº¦ã ã‘ç”¨æ„"""
        # InsightFace buffalo_l
        try:
            models_dir = self.project_root / "models_insightface" / "models" / "buffalo_l"
            if not models_dir.exists():
                import shutil, urllib.request, zipfile
                zip_path = self.project_root / "models_insightface" / "buffalo_l.zip"
                models_dir.parent.mkdir(parents=True, exist_ok=True)
                if not zip_path.exists():
                    url = "https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip"
                    print(f"Downloading buffalo_l.zip to {zip_path} ...")
                    urllib.request.urlretrieve(url, str(zip_path))
                print("Extracting buffalo_l.zip ...")
                with zipfile.ZipFile(str(zip_path), 'r') as zf:
                    zf.extractall(str(models_dir.parent))
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«äº‹å‰å–å¾—ã«å¤±æ•—ï¼ˆç¶™ç¶šï¼‰: {e}")
    
    def execute_with_eta_control(self):
        """ETAåˆ¶å¾¡ä»˜ãã§å®Ÿè¡Œ"""
        self.start_time = time.time()
        
        # ä¸­æ–­çŠ¶æ…‹ã‚’ãƒ­ãƒ¼ãƒ‰
        if self._load_resume_state():
            print("ä¸­æ–­çŠ¶æ…‹ã‹ã‚‰å†é–‹ã—ã¾ã™...")
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’å†è¨ˆç®—
            video_info = self._get_video_info()
            if not video_info:
                return False
            
            print(f"å‹•ç”»æƒ…å ±: {video_info['total_frames']} ãƒ•ãƒ¬ãƒ¼ãƒ , {video_info['fps']:.2f} FPS, {video_info['total_duration']:.1f} ç§’")
            
            chunks = self._create_chunks(video_info['total_duration'])
            # æ—¢ã«å®Œäº†ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–
            completed_ids = {cid for ok, cid, _ in self.chunk_results if ok}
            chunks_to_process = [c for c in chunks if c[0] not in completed_ids]
            print(f"å†é–‹ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯: {len(chunks_to_process)}/{len(chunks)}")
        else:
            # é€šå¸¸ã®å®Ÿè¡Œ
            video_info = self._get_video_info()
            if not video_info:
                return False
            
            print(f"å‹•ç”»æƒ…å ±: {video_info['total_frames']} ãƒ•ãƒ¬ãƒ¼ãƒ , {video_info['fps']:.2f} FPS, {video_info['total_duration']:.1f} ç§’")
            
            chunks = self._create_chunks(video_info['total_duration'])
            chunks_to_process = chunks
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            Path(self.chunks_dir).mkdir(parents=True, exist_ok=True)
        
        # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        print(f"ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†é–‹å§‹: {len(chunks_to_process)} ãƒãƒ£ãƒ³ã‚¯")
        
        # é€²æ—è¨ˆç®—ç”¨: ãƒãƒ£ãƒ³ã‚¯IDâ†’ç§’æ•°
        chunk_id_to_sec = {cid: dur for (cid, _st, dur) in chunks}
        completed_secs = 0.0
        total_secs = sum(chunk_id_to_sec.values())
        successful_chunks = []
        
        # ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆInsightFaceãªã©ï¼‰
        self._ensure_models_downloaded()
        
        with mp.Pool(processes=min(self.num_chunks, mp.cpu_count())) as pool:
            for success, chunk_id, output_csv in pool.imap_unordered(self.process_chunk_with_monitoring, chunks_to_process):
                if success:
                    successful_chunks.append(output_csv)
                    self.chunk_results.append((True, chunk_id, output_csv))
                    completed_secs += chunk_id_to_sec.get(chunk_id, 0.0)
                    # éƒ¨åˆ†ãƒãƒ¼ã‚¸ã‚’é€æ¬¡å®Ÿæ–½ï¼ˆä¸­æ–­å†é–‹ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«CSVã‚’æ›´æ–°ï¼‰
                    try:
                        self._merge_chunks(successful_chunks, self.output_csv)
                    except Exception as e:
                        print(f"éƒ¨åˆ†ãƒãƒ¼ã‚¸ã«å¤±æ•—ï¼ˆç¶™ç¶šï¼‰: {e}")
                else:
                    # å¤±æ•—ã§ã‚‚ETAè¨ˆç®—ã«å«ã‚ãªã„
                    pass
                # ETAè¡¨ç¤º
                elapsed = time.time() - self.start_time
                if completed_secs > 0:
                    rate = completed_secs / elapsed
                    remaining = max(0.0, (total_secs - completed_secs) / rate) if rate > 0 else float('inf')
                    print(f"é€²æ—: {completed_secs:.1f}/{total_secs:.1f} ç§’åˆ† å®Œäº† | çµŒé: {elapsed:.1f}s | æ¨å®šæ®‹ã‚Š: {remaining:.1f}s")
                else:
                    print(f"çµŒé: {elapsed:.1f}s | é€²æ—è¨ˆæ¸¬å¾…ã¡...")
                # éšæ™‚ä¿å­˜
                self._save_resume_state()
        
        end_time = time.time()
        processing_time = end_time - self.start_time
        
        print(f"\nğŸ‰ ä¸¦åˆ—å‡¦ç†å®Œäº†: {len(successful_chunks)}/{len(chunks_to_process)} ãƒãƒ£ãƒ³ã‚¯æˆåŠŸ")
        print(f"å‡¦ç†æ™‚é–“: {processing_time:.1f} ç§’")
        print(f"é€Ÿåº¦: {video_info['total_duration']/processing_time:.2f}x ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ")
        
        # ETAåˆ¶å¾¡çµæœ
        if self.eta_target:
            if processing_time <= self.eta_target:
                print(f"âœ… ç›®æ¨™æ™‚é–“ {self.eta_target:.1f}ç§’ å†…ã§å®Œäº†ï¼")
            else:
                print(f"âš ï¸ ç›®æ¨™æ™‚é–“ {self.eta_target:.1f}ç§’ ã‚’ {processing_time - self.eta_target:.1f}ç§’ è¶…é")
        
        # ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸
        if successful_chunks:
            self._merge_chunks(successful_chunks, self.output_csv)
        
        return len(successful_chunks) == len(chunks_to_process)
    
    def _get_video_info(self):
        """å‹•ç”»æƒ…å ±ã‚’å–å¾—"""
        # ãƒ‘ã‚¹å­˜åœ¨ç¢ºèª
        if not os.path.exists(self.video_path):
            print(f"âœ— å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.video_path}")
            return None
        # ã¾ãšOpenCVã§å–å¾—
        try:
            import cv2
            cap = cv2.VideoCapture(self.video_path)
            fps = float(cap.get(cv2.CAP_PROP_FPS) or 0)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
            if fps > 0 and total_frames > 0:
                total_duration = total_frames / fps
                cap.release()
                return {"total_frames": total_frames, "fps": fps, "total_duration": total_duration}
            cap.release()
        except Exception as e:
            print(f"OpenCVã§ã®å–å¾—ã«å¤±æ•—: {e}")
        # PyAVã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        try:
            import av
            with av.open(self.video_path) as container:
                stream = next((s for s in container.streams if s.type == 'video'), None)
                if stream is None:
                    print("âœ— PyAV: ãƒ“ãƒ‡ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return None
                # durationã¯ãƒã‚¤ã‚¯ãƒ­ç§’å˜ä½ã®ã“ã¨ãŒå¤šã„
                if container.duration:
                    total_duration = container.duration / 1e6
                elif stream.duration and stream.time_base:
                    total_duration = float(stream.duration * float(stream.time_base))
                else:
                    total_duration = 0.0
                fps = float(stream.average_rate) if stream.average_rate else 0.0
                total_frames = int(total_duration * fps) if fps > 0 and total_duration > 0 else 0
                if total_duration > 0 and fps > 0:
                    return {"total_frames": total_frames, "fps": fps, "total_duration": total_duration}
        except Exception as e:
            print(f"PyAVã§ã®å–å¾—ã«å¤±æ•—: {e}")
        print("å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—")
        return None
    
    def _create_chunks(self, total_duration):
        """ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        chunk_duration = total_duration / self.num_chunks
        chunks = []
        
        for i in range(self.num_chunks):
            start_sec = i * chunk_duration
            duration_sec = chunk_duration
            if i == self.num_chunks - 1:
                duration_sec = total_duration - start_sec
            chunks.append((i, start_sec, duration_sec))
        
        return chunks
    
    def _merge_chunks(self, chunk_files, output_file):
        """ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸"""
        print("ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸ä¸­...")
        
        import pandas as pd
        
        dfs = []
        for chunk_file in chunk_files:
            if os.path.exists(chunk_file):
                df = pd.read_csv(chunk_file)
                dfs.append(df)
        
        if dfs:
            merged_df = pd.concat(dfs, ignore_index=True)
            merged_df.to_csv(output_file, index=False)
            print(f"âœ“ ãƒãƒ¼ã‚¸å®Œäº†: {output_file}")
            print(f"  ç·æ¤œå‡ºæ•°: {len(merged_df)}")
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯äººç‰©æ•°: {merged_df['person_id'].nunique() if 'person_id' in merged_df.columns else 'N/A'}")
        else:
            print("âœ— ãƒãƒ¼ã‚¸ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

class GPUMonitor:
    """GPUç›£è¦–ã‚¯ãƒ©ã‚¹"""
    def get_stats(self):
        try:
            if GPUtil is None:
                raise RuntimeError("GPUtil not available")
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # æœ€åˆã®GPU
                return {
                    "memory_used": gpu.memoryUsed,
                    "memory_total": gpu.memoryTotal,
                    "load": gpu.load,
                    "temperature": gpu.temperature
                }
        except:
            pass
        
        return {"memory_used": 0, "memory_total": 1, "load": 0, "temperature": 0}

def main():
    parser = argparse.ArgumentParser(description="ç©¶æ¥µæ€§èƒ½ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--video", required=True, help="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--chunks", type=int, default=4, help="ãƒãƒ£ãƒ³ã‚¯æ•°")
    parser.add_argument("--config", default="ultimate", 
                       choices=["ultimate", "high_performance", "balanced", "fast"],
                       help="è¨­å®šãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--eta-target", type=float, help="ç›®æ¨™å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--output-dir", help="å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ—¢å®š: outputsï¼‰")
    parser.add_argument("--output-csv", help="ãƒãƒ¼ã‚¸å¾Œã®æœ€çµ‚CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆæ—¢å®š: <output-dir>/ultimate_merged_analysis.csvï¼‰")
    
    args = parser.parse_args()
    
    processor = UltimateParallelProcessor(
        args.video,
        args.chunks,
        args.config,
        args.eta_target,
        output_csv=args.output_csv,
        output_dir=args.output_dir,
    )
    success = processor.execute_with_eta_control()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
