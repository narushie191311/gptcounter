#!/usr/bin/env python3
"""
Colabé«˜ç²¾åº¦ãƒ»é«˜è² è·ãƒ»ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
A100ç’°å¢ƒã§ã®æœ€å¤§æ€§èƒ½ã‚’å¼•ãå‡ºã™ãŸã‚ã®æœ€é©åŒ–
"""

import os
import sys
import subprocess
import time
import multiprocessing as mp
from pathlib import Path
import argparse
import json
import psutil
import GPUtil

def get_gpu_info():
    """GPUæƒ…å ±ã‚’å–å¾—"""
    try:
        gpus = GPUtil.getGPUs()
        if gpus:
            print(f"åˆ©ç”¨å¯èƒ½GPUæ•°: {len(gpus)}")
            for i, gpu in enumerate(gpus):
                print(f"GPU {i}: {gpu.name}")
                print(f"  ãƒ¡ãƒ¢ãƒª: {gpu.memoryTotal}MB")
                print(f"  ä½¿ç”¨ç‡: {gpu.load*100:.1f}%")
                print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {gpu.memoryUsed}/{gpu.memoryTotal}MB")
        return len(gpus)
    except:
        print("GPUæƒ…å ±ã®å–å¾—ã«å¤±æ•—")
        return 0

def optimize_cuda_settings():
    """CUDAè¨­å®šã‚’æœ€é©åŒ–"""
    print("=== CUDAæœ€é©åŒ–è¨­å®š ===")
    
    # PyTorch CUDAæœ€é©åŒ–
    try:
        import torch
        if torch.cuda.is_available():
            # TF32æœ‰åŠ¹åŒ–ï¼ˆA100ã§é«˜é€ŸåŒ–ï¼‰
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            # cuDNNæœ€é©åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
            
            print("âœ“ PyTorch CUDAæœ€é©åŒ–å®Œäº†")
            print(f"  TF32: {torch.backends.cuda.matmul.allow_tf32}")
            print(f"  cuDNN benchmark: {torch.backends.cudnn.benchmark}")
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()
            
            return True
    except ImportError:
        print("PyTorchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False

def install_high_performance_deps():
    """é«˜æ€§èƒ½å‡¦ç†ç”¨ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("=== é«˜æ€§èƒ½ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ===")
    
    packages = [
        # åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
        "torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118",
        "ultralytics",
        "opencv-python",
        "numpy",
        "pandas",
        "av>=10.0.0",
        "insightface",
        "onnxruntime-gpu",
        "supervision",
        "annoy",
        "gdown",
        
        # é«˜æ€§èƒ½å‡¦ç†ç”¨
        "tensorrt",  # TensorRTï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        "nvidia-ml-py3",  # GPUç›£è¦–
        "psutil",  # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
        "GPUtil",  # GPUç›£è¦–
        "joblib",  # ä¸¦åˆ—å‡¦ç†
        "tqdm",  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
    ]
    
    for pkg in packages:
        try:
            subprocess.run(f"pip install -q {pkg}", shell=True, check=False)
            print(f"âœ“ {pkg}")
        except:
            print(f"âœ— {pkg} ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—")

def create_optimized_config():
    """æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    config = {
        "high_performance": {
            "det_size": "1536x1536",  # æœ€é«˜è§£åƒåº¦
            "yolo_weights": "yolov8x.pt",  # æœ€å¤§ãƒ¢ãƒ‡ãƒ«
            "reid_backend": "ensemble",
            "face_model": "buffalo_l",
            "gait_features": True,
            "detect_every_n": 1,
            "log_every_sec": 5,
            "checkpoint_every_sec": 15,
            "merge_every_sec": 60,
            "flush_every_n": 10,
            "use_tensorrt": True,
            "fp16_inference": True,
            "batch_size": 4,
            "num_workers": 4
        },
        "balanced": {
            "det_size": "1280x1280",
            "yolo_weights": "yolov8m.pt",
            "reid_backend": "ensemble",
            "face_model": "buffalo_l",
            "gait_features": True,
            "detect_every_n": 1,
            "log_every_sec": 10,
            "checkpoint_every_sec": 30,
            "merge_every_sec": 120,
            "flush_every_n": 20,
            "use_tensorrt": False,
            "fp16_inference": True,
            "batch_size": 2,
            "num_workers": 2
        },
        "fast": {
            "det_size": "960x960",
            "yolo_weights": "yolov8n.pt",
            "reid_backend": "hist",
            "face_model": "buffalo_l",
            "gait_features": False,
            "detect_every_n": 2,
            "log_every_sec": 15,
            "checkpoint_every_sec": 60,
            "merge_every_sec": 300,
            "flush_every_n": 50,
            "use_tensorrt": False,
            "fp16_inference": False,
            "batch_size": 1,
            "num_workers": 1
        }
    }
    
    with open("colab_config.json", "w") as f:
        json.dump(config, f, indent=2)
    
    print("âœ“ æœ€é©åŒ–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: colab_config.json ã‚’ä½œæˆ")

def create_parallel_processor():
    """ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    script_content = '''#!/usr/bin/env python3
"""
Colabä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¤‡æ•°GPU/ãƒ—ãƒ­ã‚»ã‚¹ã§ã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾
"""

import os
import sys
import subprocess
import multiprocessing as mp
import json
import time
from pathlib import Path
import argparse

def process_chunk(args):
    """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
    chunk_id, video_path, start_sec, duration_sec, config, output_dir = args
    
    output_csv = f"{output_dir}/chunk_{chunk_id:03d}.csv"
    output_video = f"{output_dir}/chunk_{chunk_id:03d}.mp4"
    
    cmd = f'''python scripts/analyze_video_mac.py \\
      --video "{video_path}" \\
      --start-sec {start_sec} \\
      --duration-sec {duration_sec} \\
      --output-csv {output_csv} \\
      --device cuda \\
      --yolo-weights {config['yolo_weights']} \\
      --reid-backend {config['reid_backend']} \\
      --face-model {config['face_model']} \\
      --det-size {config['det_size']} \\
      --detect-every-n {config['detect_every_n']} \\
      --log-every-sec {config['log_every_sec']} \\
      --checkpoint-every-sec {config['checkpoint_every_sec']} \\
      --merge-every-sec {config['merge_every_sec']} \\
      --flush-every-n {config['flush_every_n']} \\
      --save-video --video-out {output_video} \\
      --no-show'''
    
    if config.get('gait_features'):
        cmd += " --gait-features"
    
    print(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: {start_sec}s - {start_sec + duration_sec}s é–‹å§‹")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ“ ãƒãƒ£ãƒ³ã‚¯ {chunk_id} å®Œäº†")
            return True, chunk_id, output_csv
        else:
            print(f"âœ— ãƒãƒ£ãƒ³ã‚¯ {chunk_id} å¤±æ•—: {result.stderr}")
            return False, chunk_id, None
    except Exception as e:
        print(f"âœ— ãƒãƒ£ãƒ³ã‚¯ {chunk_id} ã‚¨ãƒ©ãƒ¼: {e}")
        return False, chunk_id, None

def parallel_process_video(video_path, num_chunks, config_name="high_performance"):
    """ä¸¦åˆ—å‡¦ç†ã§å‹•ç”»ã‚’è§£æ"""
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    with open("colab_config.json", "r") as f:
        configs = json.load(f)
    
    config = configs[config_name]
    
    # å‹•ç”»é•·ã‚’å–å¾—
    try:
        import cv2
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_duration = total_frames / fps
        cap.release()
    except:
        print("å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—")
        return False
    
    print(f"å‹•ç”»æƒ…å ±: {total_frames} ãƒ•ãƒ¬ãƒ¼ãƒ , {fps:.2f} FPS, {total_duration:.1f} ç§’")
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    chunk_duration = total_duration / num_chunks
    chunks = []
    
    for i in range(num_chunks):
        start_sec = i * chunk_duration
        duration_sec = chunk_duration
        if i == num_chunks - 1:  # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã¯æ®‹ã‚Šæ™‚é–“
            duration_sec = total_duration - start_sec
        
        chunks.append((i, video_path, start_sec, duration_sec, config, "outputs/chunks"))
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path("outputs/chunks").mkdir(parents=True, exist_ok=True)
    
    # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
    print(f"ä¸¦åˆ—å‡¦ç†é–‹å§‹: {num_chunks} ãƒãƒ£ãƒ³ã‚¯")
    start_time = time.time()
    
    # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
    with mp.Pool(processes=min(num_chunks, mp.cpu_count())) as pool:
        results = pool.map(process_chunk, chunks)
    
    # çµæœé›†è¨ˆ
    successful_chunks = []
    for success, chunk_id, output_csv in results:
        if success:
            successful_chunks.append(output_csv)
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"ä¸¦åˆ—å‡¦ç†å®Œäº†: {len(successful_chunks)}/{num_chunks} ãƒãƒ£ãƒ³ã‚¯æˆåŠŸ")
    print(f"å‡¦ç†æ™‚é–“: {processing_time:.1f} ç§’")
    print(f"é€Ÿåº¦: {total_duration/processing_time:.2f}x ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ")
    
    # ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸
    if successful_chunks:
        merge_chunks(successful_chunks, "outputs/merged_analysis.csv")
    
    return len(successful_chunks) == num_chunks

def merge_chunks(chunk_files, output_file):
    """ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸"""
    print("ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ãƒãƒ¼ã‚¸ä¸­...")
    
    import pandas as pd
    
    dfs = []
    for chunk_file in chunk_files:
        if os.path.exists(chunk_file):
            df = pd.read_csv(chunk_file)
            dfs.append(df)
    
    if dfs:
        merged_df = pd.concat(dfs, ignore_index=True)
        merged_df.to_csv(output_file, index=False)
        print(f"âœ“ ãƒãƒ¼ã‚¸å®Œäº†: {output_file}")
        print(f"  ç·æ¤œå‡ºæ•°: {len(merged_df)}")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯äººç‰©æ•°: {merged_df['person_id'].nunique() if 'person_id' in merged_df.columns else 'N/A'}")
    else:
        print("âœ— ãƒãƒ¼ã‚¸ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Colabä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--video", required=True, help="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--chunks", type=int, default=4, help="ãƒãƒ£ãƒ³ã‚¯æ•°")
    parser.add_argument("--config", default="high_performance", 
                       choices=["high_performance", "balanced", "fast"],
                       help="è¨­å®šãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
    
    args = parser.parse_args()
    
    success = parallel_process_video(args.video, args.chunks, args.config)
    sys.exit(0 if success else 1)
'''
    
    with open("scripts/colab_parallel.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ“ ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: scripts/colab_parallel.py ã‚’ä½œæˆ")

def create_tensorrt_optimizer():
    """TensorRTæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    script_content = '''#!/usr/bin/env python3
"""
TensorRTæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
YOLOãƒ¢ãƒ‡ãƒ«ã‚’TensorRTã‚¨ãƒ³ã‚¸ãƒ³ã«å¤‰æ›
"""

import os
import sys
import subprocess
import time
from pathlib import Path

def optimize_yolo_model(model_path, output_dir="models/tensorrt"):
    """YOLOãƒ¢ãƒ‡ãƒ«ã‚’TensorRTæœ€é©åŒ–"""
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    print(f"TensorRTæœ€é©åŒ–é–‹å§‹: {model_path}")
    
    # YOLOv8 TensorRTå¤‰æ›
    cmd = f'''python -c "
import torch
from ultralytics import YOLO

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
model = YOLO('{model_path}')
print(f'ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}')

# TensorRTå¤‰æ›
try:
    trt_model = model.export(format='engine', device=0, half=True)
    print(f'TensorRTå¤‰æ›å®Œäº†: {trt_model}')
except Exception as e:
    print(f'TensorRTå¤‰æ›å¤±æ•—: {{e}}')
    print('CPU/GPUç’°å¢ƒã‚’ç¢ºèªã—ã¦ãã ã•ã„')
"'''
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        print(result.stdout)
        if result.stderr:
            print("ã‚¨ãƒ©ãƒ¼:", result.stderr)
        
        # å¤‰æ›çµæœç¢ºèª
        trt_files = list(Path(output_dir).glob("*.engine"))
        if trt_files:
            print(f"âœ“ TensorRTã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆå®Œäº†: {len(trt_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
            for f in trt_files:
                print(f"  {f}")
        else:
            print("âœ— TensorRTã‚¨ãƒ³ã‚¸ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
    except Exception as e:
        print(f"TensorRTæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    models = [
        "yolov8n.pt",
        "yolov8m.pt", 
        "yolov8x.pt"
    ]
    
    for model in models:
        if Path(model).exists():
            print(f"\\n=== {model} ã®æœ€é©åŒ– ===")
            optimize_yolo_model(model)
        else:
            print(f"âœ— {model} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    main()
'''
    
    with open("scripts/optimize_tensorrt.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ“ TensorRTæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: scripts/optimize_tensorrt.py ã‚’ä½œæˆ")

def create_monitoring_script():
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    script_content = '''#!/usr/bin/env python3
"""
ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GPU/CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’ç›£è¦–
"""

import time
import psutil
import GPUtil
import json
from datetime import datetime

def get_system_stats():
    """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã‚’å–å¾—"""
    stats = {
        "timestamp": datetime.now().isoformat(),
        "cpu": {
            "usage_percent": psutil.cpu_percent(interval=1),
            "count": psutil.cpu_count(),
            "freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None
        },
        "memory": {
            "total": psutil.virtual_memory().total,
            "available": psutil.virtual_memory().available,
            "used": psutil.virtual_memory().used,
            "percent": psutil.virtual_memory().percent
        },
        "gpu": []
    }
    
    try:
        gpus = GPUtil.getGPUs()
        for gpu in gpus:
            stats["gpu"].append({
                "id": gpu.id,
                "name": gpu.name,
                "load": gpu.load,
                "memory_used": gpu.memoryUsed,
                "memory_total": gpu.memoryTotal,
                "temperature": gpu.temperature
            })
    except:
        pass
    
    return stats

def monitor_system(interval=5, output_file="system_monitor.json"):
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚’é–‹å§‹"""
    print(f"ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹ (é–“éš”: {interval}ç§’)")
    print("Ctrl+C ã§åœæ­¢")
    
    try:
        while True:
            stats = get_system_stats()
            
            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
            print(f"\\n=== {stats['timestamp']} ===")
            print(f"CPU: {stats['cpu']['usage_percent']:.1f}%")
            print(f"ãƒ¡ãƒ¢ãƒª: {stats['memory']['percent']:.1f}%")
            
            for gpu in stats['gpu']:
                print(f"GPU {gpu['id']}: {gpu['load']*100:.1f}% | "
                      f"ãƒ¡ãƒ¢ãƒª: {gpu['memory_used']}/{gpu['memory_total']}MB | "
                      f"æ¸©åº¦: {gpu['temperature']}Â°C")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            with open(output_file, "w") as f:
                json.dump(stats, f, indent=2)
            
            time.sleep(interval)
            
    except KeyboardInterrupt:
        print("\\nç›£è¦–åœæ­¢")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--interval", type=int, default=5, help="ç›£è¦–é–“éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--output", default="system_monitor.json", help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«")
    
    args = parser.parse_args()
    monitor_system(args.interval, args.output)
'''
    
    with open("scripts/monitor_system.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print("âœ“ ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: scripts/monitor_system.py ã‚’ä½œæˆ")

def create_high_performance_notebook():
    """é«˜æ€§èƒ½Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ"""
    notebook_content = '''# GPT Counter - Colabé«˜ç²¾åº¦ãƒ»é«˜è² è·ãƒ»ä¸¦åˆ—å‡¦ç†

## ğŸš€ é«˜æ€§èƒ½ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ç’°å¢ƒæœ€é©åŒ–
```python
# GPUç’°å¢ƒç¢ºèª
!nvidia-smi
import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU count:", torch.cuda.device_count())

# CUDAæœ€é©åŒ–
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.cuda.empty_cache()
    print("âœ“ CUDAæœ€é©åŒ–å®Œäº†")
```

### 2. é«˜æ€§èƒ½ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```python
!bash scripts/colab_quick_start.sh
!python scripts/colab_setup.py
```

### 3. TensorRTæœ€é©åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
```python
# YOLOãƒ¢ãƒ‡ãƒ«ã‚’TensorRTã‚¨ãƒ³ã‚¸ãƒ³ã«å¤‰æ›
!python scripts/optimize_tensorrt.py
```

## âš¡ é«˜ç²¾åº¦ãƒ»é«˜è² è·è§£æ

### æœ€é«˜ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰ï¼ˆA100æ¨å¥¨ï¼‰
```python
VIDEO_PATH = "YOUR_VIDEO_PATH"  # ãƒ‘ã‚¹ã‚’è¨­å®š

# æœ€é«˜ç²¾åº¦è¨­å®š
!python scripts/analyze_video_mac.py \\
  --video "$VIDEO_PATH" \\
  --start-sec 0 --duration-sec 0 \\
  --output-csv outputs/analysis_hp.csv \\
  --device cuda \\
  --yolo-weights yolov8x.pt \\
  --reid-backend ensemble \\
  --face-model buffalo_l \\
  --gait-features \\
  --det-size 1536x1536 \\
  --detect-every-n 1 \\
  --log-every-sec 5 \\
  --checkpoint-every-sec 15 \\
  --merge-every-sec 60 \\
  --flush-every-n 10
```

### ä¸¦åˆ—å‡¦ç†ï¼ˆãƒãƒ«ãƒãƒãƒ£ãƒ³ã‚¯ï¼‰
```python
# 4ä¸¦åˆ—ã§é«˜é€Ÿå‡¦ç†
!python scripts/colab_parallel.py \\
  --video "$VIDEO_PATH" \\
  --chunks 4 \\
  --config high_performance

# 8ä¸¦åˆ—ã§è¶…é«˜è² è·å‡¦ç†ï¼ˆA100ç’°å¢ƒï¼‰
!python scripts/colab_parallel.py \\
  --video "$VIDEO_PATH" \\
  --chunks 8 \\
  --config high_performance
```

### ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
```python
# åˆ¥ã‚»ãƒ«ã§å®Ÿè¡Œï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ç›£è¦–ï¼‰
!python scripts/monitor_system.py --interval 3 --output system_stats.json

# ç›£è¦–çµæœç¢ºèª
import json
with open("system_stats.json", "r") as f:
    stats = json.load(f)
print(f"GPUä½¿ç”¨ç‡: {stats['gpu'][0]['load']*100:.1f}%")
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {stats['memory']['percent']:.1f}%")
```

## ğŸ”¥ æ¥µé™æœ€é©åŒ–è¨­å®š

### è¶…é«˜è§£åƒåº¦å‡¦ç†
```python
# 2048x2048è§£åƒåº¦ï¼ˆA100å°‚ç”¨ï¼‰
!python scripts/analyze_video_mac.py \\
  --video "$VIDEO_PATH" \\
  --det-size 2048x2048 \\
  --yolo-weights yolov8x.pt \\
  --device cuda \\
  --reid-backend ensemble \\
  --face-model buffalo_l \\
  --gait-features \\
  --detect-every-n 1 \\
  --log-every-sec 2 \\
  --checkpoint-every-sec 10
```

### ãƒãƒ«ãƒGPUå‡¦ç†
```python
# GPUæ•°ã«å¿œã˜ãŸä¸¦åˆ—å‡¦ç†
import torch
gpu_count = torch.cuda.device_count()

if gpu_count > 1:
    print(f"ãƒãƒ«ãƒGPUå‡¦ç†: {gpu_count} GPU")
    # å„GPUã§ç•°ãªã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
    for gpu_id in range(gpu_count):
        start_chunk = gpu_id * (total_chunks // gpu_count)
        end_chunk = (gpu_id + 1) * (total_chunks // gpu_count)
        print(f"GPU {gpu_id}: ãƒãƒ£ãƒ³ã‚¯ {start_chunk}-{end_chunk}")
```

### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
```python
# ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–
import torch

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ã—ãªãŒã‚‰å‡¦ç†
def process_with_memory_management():
    torch.cuda.empty_cache()
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„èª¿æ•´
    batch_size = 4
    while batch_size > 0:
        try:
            # å‡¦ç†å®Ÿè¡Œ
            torch.cuda.empty_cache()
            break
        except torch.cuda.OutOfMemoryError:
            batch_size //= 2
            torch.cuda.empty_cache()
            print(f"ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ {batch_size} ã«èª¿æ•´")
```

## ğŸ“Š æ€§èƒ½æ¸¬å®šã¨æ¯”è¼ƒ

### å‡¦ç†é€Ÿåº¦æ¸¬å®š
```python
import time
import pandas as pd

def measure_performance(video_path, configs):
    results = []
    
    for config_name, config in configs.items():
        start_time = time.time()
        
        # è§£æå®Ÿè¡Œ
        cmd = f"python scripts/analyze_video_mac.py --video {video_path} " + \\
              " ".join([f"--{k} {v}" for k, v in config.items()])
        
        subprocess.run(cmd, shell=True)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        results.append({
            "config": config_name,
            "processing_time": processing_time,
            "speedup": total_duration / processing_time
        })
    
    return pd.DataFrame(results)

# è¨­å®šæ¯”è¼ƒ
configs = {
    "fast": {"det-size": "640x640", "yolo-weights": "yolov8n.pt"},
    "balanced": {"det-size": "1280x1280", "yolo-weights": "yolov8m.pt"},
    "high_performance": {"det-size": "1536x1536", "yolo-weights": "yolov8x.pt"}
}

performance_df = measure_performance(VIDEO_PATH, configs)
print(performance_df)
```

### çµæœã®å¯è¦–åŒ–
```python
import matplotlib.pyplot as plt

# æ€§èƒ½æ¯”è¼ƒã‚°ãƒ©ãƒ•
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# å‡¦ç†æ™‚é–“æ¯”è¼ƒ
performance_df.plot(x="config", y="processing_time", kind="bar", ax=ax1)
ax1.set_title("å‡¦ç†æ™‚é–“æ¯”è¼ƒ")
ax1.set_ylabel("å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰")

# é€Ÿåº¦å‘ä¸Šç‡æ¯”è¼ƒ
performance_df.plot(x="config", y="speedup", kind="bar", ax=ax2)
ax2.set_title("é€Ÿåº¦å‘ä¸Šç‡æ¯”è¼ƒ")
ax2.set_ylabel("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¯”")

plt.tight_layout()
plt.show()
```

## ğŸ¯ æ¨å¥¨å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³

### åˆå›å®Ÿè¡Œï¼ˆå‹•ä½œç¢ºèªï¼‰
```python
# è»½é‡è¨­å®šã§ãƒ†ã‚¹ãƒˆ
!python scripts/colab_parallel.py \\
  --video "$VIDEO_PATH" \\
  --chunks 2 \\
  --config fast
```

### æœ¬æ ¼å®Ÿè¡Œï¼ˆé«˜ç²¾åº¦ï¼‰
```python
# é«˜ç²¾åº¦è¨­å®šã§æœ¬æ ¼å‡¦ç†
!python scripts/colab_parallel.py \\
  --video "$VIDEO_PATH" \\
  --chunks 4 \\
  --config high_performance
```

### æ¥µé™å®Ÿè¡Œï¼ˆA100ç’°å¢ƒï¼‰
```python
# æœ€é«˜æ€§èƒ½è¨­å®šï¼ˆA100å°‚ç”¨ï¼‰
!python scripts/colab_parallel.py \\
  --video "$VIDEO_PATH" \\
  --chunks 8 \\
  --config high_performance

# ã‚«ã‚¹ã‚¿ãƒ è¶…é«˜è§£åƒåº¦
!python scripts/analyze_video_mac.py \\
  --video "$VIDEO_PATH" \\
  --det-size 2048x2048 \\
  --yolo-weights yolov8x.pt \\
  --device cuda \\
  --reid-backend ensemble \\
  --face-model buffalo_l \\
  --gait-features
```

## âš ï¸ æ³¨æ„äº‹é …

1. **ãƒ¡ãƒ¢ãƒªç®¡ç†**: é«˜è§£åƒåº¦å‡¦ç†æ™‚ã¯GPUãƒ¡ãƒ¢ãƒªã‚’ç›£è¦–
2. **æ¸©åº¦ç®¡ç†**: é•·æ™‚é–“å®Ÿè¡Œæ™‚ã¯GPUæ¸©åº¦ã‚’ç¢ºèª
3. **ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ **: ä¸­æ–­æ™‚ã¯åŒã˜ã‚³ãƒãƒ³ãƒ‰ã§å†é–‹å¯èƒ½
4. **ãƒ­ã‚°ç›£è¦–**: å‡¦ç†çŠ¶æ³ã‚’å®šæœŸçš„ã«ç¢ºèª

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½å‘ä¸Š

| è¨­å®š | è§£åƒåº¦ | ä¸¦åˆ—æ•° | æœŸå¾…é€Ÿåº¦å‘ä¸Š | å‚™è€ƒ |
|------|--------|--------|-------------|------|
| Fast | 640x640 | 4 | 3-4x | è»½é‡å‡¦ç† |
| Balanced | 1280x1280 | 4 | 2-3x | ãƒãƒ©ãƒ³ã‚¹é‡è¦– |
| High Performance | 1536x1536 | 4 | 1.5-2x | é«˜ç²¾åº¦å‡¦ç† |
| Ultra | 2048x2048 | 8 | 1-1.5x | A100å°‚ç”¨ |
'''
    
    with open("gptcounter_colab_high_performance.ipynb", "w", encoding="utf-8") as f:
        f.write(notebook_content)
    
    print("âœ“ é«˜æ€§èƒ½Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: gptcounter_colab_high_performance.ipynb ã‚’ä½œæˆ")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("=== Colabé«˜ç²¾åº¦ãƒ»é«˜è² è·ãƒ»ä¸¦åˆ—å‡¦ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ ===")
    
    # GPUæƒ…å ±ç¢ºèª
    gpu_count = get_gpu_info()
    
    # CUDAæœ€é©åŒ–
    optimize_cuda_settings()
    
    # é«˜æ€§èƒ½ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    install_high_performance_deps()
    
    # æœ€é©åŒ–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    create_optimized_config()
    
    # ä¸¦åˆ—å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    create_parallel_processor()
    
    # TensorRTæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    create_tensorrt_optimizer()
    
    # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    create_monitoring_script()
    
    # é«˜æ€§èƒ½Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä½œæˆ
    create_high_performance_notebook()
    
    print("\n=== é«˜ç²¾åº¦ãƒ»é«˜è² è·ãƒ»ä¸¦åˆ—å‡¦ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº† ===")
    print("\nğŸ¯ æ¨å¥¨å®Ÿè¡Œæ‰‹é †:")
    print("1. gptcounter_colab_high_performance.ipynb ã‚’Colabã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    print("2. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ â†’ GPU (A100æ¨å¥¨)")
    print("3. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‚’å®Ÿè¡Œ")
    print("4. ä¸¦åˆ—å‡¦ç†ã§é«˜ç²¾åº¦è§£æå®Ÿè¡Œ")
    print("\nâš¡ æ€§èƒ½å‘ä¸Šã®ãƒã‚¤ãƒ³ãƒˆ:")
    print(f"- ä¸¦åˆ—å‡¦ç†: {gpu_count} GPUç’°å¢ƒã§æœ€å¤§{gpu_count*2}ä¸¦åˆ—æ¨å¥¨")
    print("- é«˜è§£åƒåº¦: 1536x1536ä»¥ä¸Šã§é«˜ç²¾åº¦å‡¦ç†")
    print("- TensorRT: åˆ©ç”¨å¯èƒ½ãªå ´åˆã¯è‡ªå‹•æœ€é©åŒ–")
    print("- ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–: ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã‚’ç›£è¦–")

if __name__ == "__main__":
    main()
